{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9df9b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRIYANGSHU\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f23814",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8f8c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracion_dict={\n",
    "    \"aren't\": [\"are not\"],\n",
    "    \"can't\": [\"cannot\"],\n",
    "    \"couldn't\": [\"could not\"],\n",
    "    \"didn't\": [\"did not\"],\n",
    "    \"doesn't\": [\"does not\"],\n",
    "    \"don't\": [\"do not\"],\n",
    "    \"hadn't\": [\"had not\"],\n",
    "    \"hasn't\": [\"has not\"],\n",
    "    \"haven't\": [\"have not\"],\n",
    "    \"he'd\": [\"he had\", \"he would\"],\n",
    "    \"he'll\": [\"he will\", \"he shall\"],\n",
    "    \"he's\": [\"he is\", \"he has\"],\n",
    "    \"I'd\": [\"I had\", \"I would\"],\n",
    "    \"I'll\": [\"I will\", \"I shall\"],\n",
    "    \"I'm\": [\"I am\"],\n",
    "    \"I've\": [\"I have\"],\n",
    "    \"isn't\": [\"is not\"],\n",
    "    \"it'd\": [\"it had\", \"it would\"],\n",
    "    \"it'll\": [\"it will\", \"it shall\"],\n",
    "    \"it's\": [\"it is\", \"it has\"],\n",
    "    \"let's\": [\"let us\"],\n",
    "    \"mightn't\": [\"might not\"],\n",
    "    \"mustn't\": [\"must not\"],\n",
    "    \"shan't\": [\"shall not\"],\n",
    "    \"she'd\": [\"she had\", \"she would\"],\n",
    "    \"she'll\": [\"she will\", \"she shall\"],\n",
    "    \"she's\": [\"she is\", \"she has\"],\n",
    "    \"shouldn't\": [\"should not\"],\n",
    "    \"that's\": [\"that is\", \"that has\"],\n",
    "    \"there's\": [\"there is\", \"there has\"],\n",
    "    \"they'd\": [\"they had\", \"they would\"],\n",
    "    \"they'll\": [\"they will\", \"they shall\"],\n",
    "    \"they're\": [\"they are\"],\n",
    "    \"they've\": [\"they have\"],\n",
    "    \"wasn't\": [\"was not\"],\n",
    "    \"we'd\": [\"we had\", \"we would\"],\n",
    "    \"we're\": [\"we are\"],\n",
    "    \"we've\": [\"we have\"],\n",
    "    \"weren't\": [\"were not\"],\n",
    "    \"what'll\": [\"what will\", \"what shall\"],\n",
    "    \"what're\": [\"what are\"],\n",
    "    \"what's\": [\"what is\", \"what has\"],\n",
    "    \"what've\": [\"what have\"],\n",
    "    \"where's\": [\"where is\", \"where has\"],\n",
    "    \"who'd\": [\"who had\", \"who would\"],\n",
    "    \"who'll\": [\"who will\", \"who shall\"],\n",
    "    \"who's\": [\"who is\", \"who has\"],\n",
    "    \"won't\": [\"will not\"],\n",
    "    \"wouldn't\": [\"would not\"],\n",
    "    \"you'd\": [\"you had\", \"you would\"],\n",
    "    \"you'll\": [\"you will\", \"you shall\"],\n",
    "    \"you're\": [\"you are\"],\n",
    "    \"you've\": [\"you have\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d4437ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1=re.compile(r\"[a-zA-z]+\\.\")\n",
    "pattern2=re.compile(r\"[a-zA-Z]+'[a-zA-z]*\")\n",
    "pattern3=re.compile(r\"'[a-zA-z0-9]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "724ded63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aren't\n",
      "can't\n",
      "couldn't\n",
      "didn't\n",
      "doesn't\n",
      "don't\n",
      "hadn't\n",
      "hasn't\n",
      "haven't\n",
      "he'd\n",
      "he'll\n",
      "he's\n",
      "I'd\n",
      "I'll\n",
      "I'm\n",
      "I've\n",
      "isn't\n",
      "it'd\n",
      "it'll\n",
      "it's\n",
      "let's\n",
      "mightn't\n",
      "mustn't\n",
      "shan't\n",
      "she'd\n",
      "she'll\n",
      "she's\n",
      "shouldn't\n",
      "that's\n",
      "there's\n",
      "they'd\n",
      "they'll\n",
      "they're\n",
      "they've\n",
      "wasn't\n",
      "we'd\n",
      "we're\n",
      "we've\n",
      "weren't\n",
      "what'll\n",
      "what're\n",
      "what's\n",
      "what've\n",
      "where's\n",
      "who'd\n",
      "who'll\n",
      "who's\n",
      "won't\n",
      "wouldn't\n",
      "you'd\n",
      "you'll\n",
      "you're\n",
      "you've\n"
     ]
    }
   ],
   "source": [
    "list1=[]\n",
    "for key in contracion_dict:\n",
    "   m1=re.match(pattern1,key)\n",
    "   m2=re.match(pattern2,key)\n",
    "   m3=re.match(pattern3,key)\n",
    "   if m1 or m2 or m3:\n",
    "    if m1:\n",
    "      print(m1.group())\n",
    "    if m2:\n",
    "      print(m2.group())\n",
    "    if m3:\n",
    "      print(m3.group())\n",
    "    list1.append(False)\n",
    "   else:\n",
    "      list1.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "757b41d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde3b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MytokenizerV1:\n",
    "    def __init__(self):\n",
    "        \n",
    "       self.contraction_dict = {\n",
    "    \"aren't\": [\"are not\"],\n",
    "    \"can't\": [\"cannot\"],\n",
    "    \"couldn't\": [\"could not\"],\n",
    "    \"didn't\": [\"did not\"],\n",
    "    \"doesn't\": [\"does not\"],\n",
    "    \"don't\": [\"do not\"],\n",
    "    \"hadn't\": [\"had not\"],\n",
    "    \"hasn't\": [\"has not\"],\n",
    "    \"haven't\": [\"have not\"],\n",
    "    \"he'd\": [\"he had\", \"he would\"],\n",
    "    \"he'll\": [\"he will\", \"he shall\"],\n",
    "    \"he's\": [\"he is\", \"he has\"],\n",
    "    \"I'd\": [\"I had\", \"I would\"],\n",
    "    \"I'll\": [\"I will\", \"I shall\"],\n",
    "    \"I'm\": [\"I am\"],\n",
    "    \"I've\": [\"I have\"],\n",
    "    \"isn't\": [\"is not\"],\n",
    "    \"it'd\": [\"it had\", \"it would\"],\n",
    "    \"it'll\": [\"it will\", \"it shall\"],\n",
    "    \"it's\": [\"it is\", \"it has\"],\n",
    "    \"let's\": [\"let us\"],\n",
    "    \"mightn't\": [\"might not\"],\n",
    "    \"mustn't\": [\"must not\"],\n",
    "    \"shan't\": [\"shall not\"],\n",
    "    \"she'd\": [\"she had\", \"she would\"],\n",
    "    \"she'll\": [\"she will\", \"she shall\"],\n",
    "    \"she's\": [\"she is\", \"she has\"],\n",
    "    \"shouldn't\": [\"should not\"],\n",
    "    \"that's\": [\"that is\", \"that has\"],\n",
    "    \"there's\": [\"there is\", \"there has\"],\n",
    "    \"they'd\": [\"they had\", \"they would\"],\n",
    "    \"they'll\": [\"they will\", \"they shall\"],\n",
    "    \"they're\": [\"they are\"],\n",
    "    \"they've\": [\"they have\"],\n",
    "    \"wasn't\": [\"was not\"],\n",
    "    \"we'd\": [\"we had\", \"we would\"],\n",
    "    \"we're\": [\"we are\"],\n",
    "    \"we've\": [\"we have\"],\n",
    "    \"weren't\": [\"were not\"],\n",
    "    \"what'll\": [\"what will\", \"what shall\"],\n",
    "    \"what're\": [\"what are\"],\n",
    "    \"what's\": [\"what is\", \"what has\"],\n",
    "    \"what've\": [\"what have\"],\n",
    "    \"where's\": [\"where is\", \"where has\"],\n",
    "    \"who'd\": [\"who had\", \"who would\"],\n",
    "    \"who'll\": [\"who will\", \"who shall\"],\n",
    "    \"who's\": [\"who is\", \"who has\"],\n",
    "    \"won't\": [\"will not\"],\n",
    "    \"wouldn't\": [\"would not\"],\n",
    "    \"you'd\": [\"you had\", \"you would\"],\n",
    "    \"you'll\": [\"you will\", \"you shall\"],\n",
    "    \"you're\": [\"you are\"],\n",
    "    \"you've\": [\"you have\"],\n",
    "}\n",
    "\n",
    "        \n",
    "\n",
    "    def tokenize(self,sent):\n",
    "        pattern2=re.compile(r\"[a-zA-Z]+'[a-zA-z]*\") \n",
    "        match_abr=re.finditer(pattern2,sent)\n",
    "        \n",
    "        for match in match_abr:\n",
    "            print(match.group())\n",
    "            if (match.group().strip().lower()) in self.contraction_dict:\n",
    "\n",
    "                if (len(self.contraction_dict[match.group().strip().lower()])<2):\n",
    "                    sent=re.sub(match.group(),self.contraction_dict[match.group().strip().lower()][0],sent)\n",
    "                else:\n",
    "                    continue\n",
    "            elif (match.group().strip()) in self.contraction_dict :\n",
    "                if (len(self.contraction_dict[match.group().strip()])<2):\n",
    "                    sent=re.sub(match.group(),self.contraction_dict[match.group().strip()][0],sent)\n",
    "            else:\n",
    "                continue\n",
    "        return sent.split()\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8fd4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm\n",
      "It's\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Priyanshu',\n",
       " 'Das.',\n",
       " \"It's\",\n",
       " 'nice',\n",
       " 'of',\n",
       " 'you',\n",
       " 'guys',\n",
       " 'to',\n",
       " 'check',\n",
       " 'out',\n",
       " 'my',\n",
       " 'tokenizer',\n",
       " 'class.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1=MytokenizerV1()\n",
    "token1.tokenize(\"Hi I'm Priyanshu Das. It's nice of you guys to check out my tokenizer class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21906b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MytokenizerV2:\n",
    "    def __init__(self):\n",
    "        self.contracion_dict = {\n",
    "            \"I'm\": [\"I am\"],\n",
    "            \"I've\": [\"I have\"],\n",
    "            \"I'd\": [\n",
    "                {\"VB\": \"I would\"},\n",
    "                {\"VBN\": \"I had\"}\n",
    "            ],\n",
    "            \"you'd\": [\n",
    "                {\"VB\": \"you would\"},\n",
    "                {\"VBN\": \"you had\"}\n",
    "            ],\n",
    "            \"he'd\": [\n",
    "                {\"VB\": \"he would\"},\n",
    "                {\"VBN\": \"he had\"}\n",
    "            ],\n",
    "            \"she'd\": [\n",
    "                {\"VB\": \"she would\"},\n",
    "                {\"VBN\": \"she had\"}\n",
    "            ],\n",
    "            \"they'd\": [\n",
    "                {\"VB\": \"they would\"},\n",
    "                {\"VBN\": \"they had\"}\n",
    "            ],\n",
    "            \"we'd\": [\n",
    "                {\"VB\": \"we would\"},\n",
    "                {\"VBN\": \"we had\"}\n",
    "            ],\n",
    "            \"you've\": [\"you have\"],\n",
    "            \"we've\": [\"we have\"],\n",
    "            \"they've\": [\"they have\"],\n",
    "            \"you'll\": [\"you will\"],\n",
    "            \"he'll\": [\"he will\"],\n",
    "            \"she'll\": [\"she will\"],\n",
    "            \"they'll\": [\"they will\"],\n",
    "            \"we'll\": [\"we will\"],\n",
    "            \"I'll\": [\"I will\"],\n",
    "            \"isn't\": [\"is not\"],\n",
    "            \"aren't\": [\"are not\"],\n",
    "            \"wasn't\": [\"was not\"],\n",
    "            \"weren't\": [\"were not\"],\n",
    "            \"hasn't\": [\"has not\"],\n",
    "            \"haven't\": [\"have not\"],\n",
    "            \"hadn't\": [\"had not\"],\n",
    "            \"won't\": [\"will not\"],\n",
    "            \"wouldn't\": [\"would not\"],\n",
    "            \"can't\": [\"cannot\"],\n",
    "            \"couldn't\": [\"could not\"],\n",
    "            \"shouldn't\": [\"should not\"],\n",
    "            \"mustn't\": [\"must not\"],\n",
    "            \"don't\": [\"do not\"],\n",
    "            \"doesn't\": [\"does not\"],\n",
    "            \"didn't\": [\"did not\"],\n",
    "            \"it'd\": [\n",
    "                {\"VB\": \"it would\"},\n",
    "                {\"VBN\": \"it had\"}\n",
    "            ],\n",
    "            \"that's\": [\n",
    "                {\"NN\": \"that is\"},\n",
    "                {\"JJ\": \"that is\"},\n",
    "                {\"DT\": \"that is\"},\n",
    "                {\"VBN\": \"that has\"}\n",
    "            ],\n",
    "            \"what's\": [\n",
    "                {\"NN\": \"what is\"},\n",
    "                {\"JJ\": \"what is\"},\n",
    "                {\"VBN\": \"what has\"}\n",
    "            ],\n",
    "            \"who's\": [\n",
    "                {\"NN\": \"who is\"},\n",
    "                {\"VBN\": \"who has\"}\n",
    "            ],\n",
    "            \"there's\": [\n",
    "                {\"NN\": \"there is\"},\n",
    "                {\"VBN\": \"there has\"}\n",
    "            ],\n",
    "            \"where's\": [\n",
    "                {\"NN\": \"where is\"},\n",
    "                {\"VBN\": \"where has\"}\n",
    "            ],\n",
    "            \"'s\": [\n",
    "                {\"VB\": \"is\"},\n",
    "                {\"VBN\": \"has\"},\n",
    "                {\"NN\": \"POS\"}\n",
    "            ],\n",
    "            \"'d\": [\n",
    "                {\"VB\": \"would\"},\n",
    "                {\"VBN\": \"had\"}\n",
    "            ],\n",
    "            \"'ll\": [\"will\"],\n",
    "            \"'re\": [\"are\"],\n",
    "            \"'ve\": [\"have\"],\n",
    "            \"'m\": [\"am\"],\n",
    "            \"n't\": [\"not\"]\n",
    "        }\n",
    "\n",
    "        import spacy\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def get_next_pos(self, sentence, contraction):\n",
    "        doc = self.nlp(sentence)\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.text.lower() == contraction.lower():\n",
    "                if i + 1 < len(doc):\n",
    "                    next_token = doc[i + 1]\n",
    "                    return next_token.tag_  # Return fine-grained POS tag\n",
    "        return None\n",
    "\n",
    "    def tokenize(self, sent):\n",
    "        import re\n",
    "        doc = self.nlp(sent)\n",
    "        pattern2 = re.compile(r\"[a-zA-Z]+('[a-zA-Z]+)?\")\n",
    "        tokens = sent.split()\n",
    "        result = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            lower_tok = token.lower().strip(\".,?!\")\n",
    "            if lower_tok in self.contracion_dict:\n",
    "                expansion = self.contracion_dict[lower_tok]\n",
    "                if isinstance(expansion[0], str):\n",
    "                    result.extend(expansion[0].split())\n",
    "                else:\n",
    "                    pos = self.get_next_pos(sent, token)\n",
    "                    substituted = False\n",
    "                    for option in expansion:\n",
    "                        if pos in option:\n",
    "                            result.extend(option[pos].split())\n",
    "                            substituted = True\n",
    "                            break\n",
    "                    if not substituted:\n",
    "                        result.append(token)\n",
    "            else:\n",
    "                result.append(token)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c78d2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"I'm learning Python.\",\n",
    "    \"He's gone to the market.\",\n",
    "    \"They've completed their task.\",\n",
    "    \"It’s been a long time.\",\n",
    "    \"I'd seen that movie before.\",\n",
    "    \"He'd like to join us.\",\n",
    "    \"She'd eaten already.\",\n",
    "    \"You'll do great!\",\n",
    "    \"We'd be honored.\",\n",
    "    \"There’s something wrong.\",\n",
    "    \"That's amazing!\",\n",
    "    \"Who's there?\",\n",
    "    \"Don't forget to call.\",\n",
    "    \"It wouldn't work.\",\n",
    "    \"They’re on their way.\",\n",
    "    \"We’ve tried everything.\",\n",
    "    \"You'd better leave.\",\n",
    "    \"What’s happening?\",\n",
    "    \"Let's go now!\",\n",
    "    \"He’ll never agree.\",\n",
    "    \"She'll come tomorrow.\",\n",
    "    \"What've you done?\",\n",
    "    \"He hasn't replied.\",\n",
    "    \"We weren't informed.\",\n",
    "    \"Isn't it obvious?\",\n",
    "    \"Couldn’t you see it?\",\n",
    "    \"Wasn’t it nice?\",\n",
    "    \"They’d already left.\",\n",
    "    \"Wouldn’t you agree?\",\n",
    "    \"Haven’t we met before?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8bc9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: I'm learning Python.\n",
      "I'm\n",
      "Tokens1: ['I', 'am', 'learning', 'Python.']\n",
      "Tokens2: [\"I'm\", 'learning', 'Python.']\n",
      "--------------------------------------------------\n",
      "Sentence 2: He's gone to the market.\n",
      "He's\n",
      "Tokens1: [\"He's\", 'gone', 'to', 'the', 'market.']\n",
      "Tokens2: [\"He's\", 'gone', 'to', 'the', 'market.']\n",
      "--------------------------------------------------\n",
      "Sentence 3: They've completed their task.\n",
      "They've\n",
      "Tokens1: ['they', 'have', 'completed', 'their', 'task.']\n",
      "Tokens2: ['they', 'have', 'completed', 'their', 'task.']\n",
      "--------------------------------------------------\n",
      "Sentence 4: It’s been a long time.\n",
      "Tokens1: ['It’s', 'been', 'a', 'long', 'time.']\n",
      "Tokens2: ['It’s', 'been', 'a', 'long', 'time.']\n",
      "--------------------------------------------------\n",
      "Sentence 5: I'd seen that movie before.\n",
      "I'd\n",
      "Tokens1: [\"I'd\", 'seen', 'that', 'movie', 'before.']\n",
      "Tokens2: [\"I'd\", 'seen', 'that', 'movie', 'before.']\n",
      "--------------------------------------------------\n",
      "Sentence 6: He'd like to join us.\n",
      "He'd\n",
      "Tokens1: [\"He'd\", 'like', 'to', 'join', 'us.']\n",
      "Tokens2: [\"He'd\", 'like', 'to', 'join', 'us.']\n",
      "--------------------------------------------------\n",
      "Sentence 7: She'd eaten already.\n",
      "She'd\n",
      "Tokens1: [\"She'd\", 'eaten', 'already.']\n",
      "Tokens2: [\"She'd\", 'eaten', 'already.']\n",
      "--------------------------------------------------\n",
      "Sentence 8: You'll do great!\n",
      "You'll\n",
      "Tokens1: [\"You'll\", 'do', 'great!']\n",
      "Tokens2: ['you', 'will', 'do', 'great!']\n",
      "--------------------------------------------------\n",
      "Sentence 9: We'd be honored.\n",
      "We'd\n",
      "Tokens1: [\"We'd\", 'be', 'honored.']\n",
      "Tokens2: [\"We'd\", 'be', 'honored.']\n",
      "--------------------------------------------------\n",
      "Sentence 10: There’s something wrong.\n",
      "Tokens1: ['There’s', 'something', 'wrong.']\n",
      "Tokens2: ['There’s', 'something', 'wrong.']\n",
      "--------------------------------------------------\n",
      "Sentence 11: That's amazing!\n",
      "That's\n",
      "Tokens1: [\"That's\", 'amazing!']\n",
      "Tokens2: [\"That's\", 'amazing!']\n",
      "--------------------------------------------------\n",
      "Sentence 12: Who's there?\n",
      "Who's\n",
      "Tokens1: [\"Who's\", 'there?']\n",
      "Tokens2: [\"Who's\", 'there?']\n",
      "--------------------------------------------------\n",
      "Sentence 13: Don't forget to call.\n",
      "Don't\n",
      "Tokens1: ['do', 'not', 'forget', 'to', 'call.']\n",
      "Tokens2: ['do', 'not', 'forget', 'to', 'call.']\n",
      "--------------------------------------------------\n",
      "Sentence 14: It wouldn't work.\n",
      "wouldn't\n",
      "Tokens1: ['It', 'would', 'not', 'work.']\n",
      "Tokens2: ['It', 'would', 'not', 'work.']\n",
      "--------------------------------------------------\n",
      "Sentence 15: They’re on their way.\n",
      "Tokens1: ['They’re', 'on', 'their', 'way.']\n",
      "Tokens2: ['They’re', 'on', 'their', 'way.']\n",
      "--------------------------------------------------\n",
      "Sentence 16: We’ve tried everything.\n",
      "Tokens1: ['We’ve', 'tried', 'everything.']\n",
      "Tokens2: ['We’ve', 'tried', 'everything.']\n",
      "--------------------------------------------------\n",
      "Sentence 17: You'd better leave.\n",
      "You'd\n",
      "Tokens1: [\"You'd\", 'better', 'leave.']\n",
      "Tokens2: [\"You'd\", 'better', 'leave.']\n",
      "--------------------------------------------------\n",
      "Sentence 18: What’s happening?\n",
      "Tokens1: ['What’s', 'happening?']\n",
      "Tokens2: ['What’s', 'happening?']\n",
      "--------------------------------------------------\n",
      "Sentence 19: Let's go now!\n",
      "Let's\n",
      "Tokens1: ['let', 'us', 'go', 'now!']\n",
      "Tokens2: [\"Let's\", 'go', 'now!']\n",
      "--------------------------------------------------\n",
      "Sentence 20: He’ll never agree.\n",
      "Tokens1: ['He’ll', 'never', 'agree.']\n",
      "Tokens2: ['He’ll', 'never', 'agree.']\n",
      "--------------------------------------------------\n",
      "Sentence 21: She'll come tomorrow.\n",
      "She'll\n",
      "Tokens1: [\"She'll\", 'come', 'tomorrow.']\n",
      "Tokens2: ['she', 'will', 'come', 'tomorrow.']\n",
      "--------------------------------------------------\n",
      "Sentence 22: What've you done?\n",
      "What've\n",
      "Tokens1: ['what', 'have', 'you', 'done?']\n",
      "Tokens2: [\"What've\", 'you', 'done?']\n",
      "--------------------------------------------------\n",
      "Sentence 23: He hasn't replied.\n",
      "hasn't\n",
      "Tokens1: ['He', 'has', 'not', 'replied.']\n",
      "Tokens2: ['He', 'has', 'not', 'replied.']\n",
      "--------------------------------------------------\n",
      "Sentence 24: We weren't informed.\n",
      "weren't\n",
      "Tokens1: ['We', 'were', 'not', 'informed.']\n",
      "Tokens2: ['We', 'were', 'not', 'informed.']\n",
      "--------------------------------------------------\n",
      "Sentence 25: Isn't it obvious?\n",
      "Isn't\n",
      "Tokens1: ['is', 'not', 'it', 'obvious?']\n",
      "Tokens2: ['is', 'not', 'it', 'obvious?']\n",
      "--------------------------------------------------\n",
      "Sentence 26: Couldn’t you see it?\n",
      "Tokens1: ['Couldn’t', 'you', 'see', 'it?']\n",
      "Tokens2: ['Couldn’t', 'you', 'see', 'it?']\n",
      "--------------------------------------------------\n",
      "Sentence 27: Wasn’t it nice?\n",
      "Tokens1: ['Wasn’t', 'it', 'nice?']\n",
      "Tokens2: ['Wasn’t', 'it', 'nice?']\n",
      "--------------------------------------------------\n",
      "Sentence 28: They’d already left.\n",
      "Tokens1: ['They’d', 'already', 'left.']\n",
      "Tokens2: ['They’d', 'already', 'left.']\n",
      "--------------------------------------------------\n",
      "Sentence 29: Wouldn’t you agree?\n",
      "Tokens1: ['Wouldn’t', 'you', 'agree?']\n",
      "Tokens2: ['Wouldn’t', 'you', 'agree?']\n",
      "--------------------------------------------------\n",
      "Sentence 30: Haven’t we met before?\n",
      "Tokens1: ['Haven’t', 'we', 'met', 'before?']\n",
      "Tokens2: ['Haven’t', 'we', 'met', 'before?']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = MytokenizerV1()\n",
    "tokenizer2 = MytokenizerV2()\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")\n",
    "    tokens1 = tokenizer1.tokenize(sentence)\n",
    "    tokens2= tokenizer2.tokenize(sentence)\n",
    "    print(\"Tokens1:\", tokens1)\n",
    "    print(\"Tokens2:\", tokens2)\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
